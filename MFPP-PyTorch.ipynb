{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/qing/wind/MFPP/dataset/imagenet/imagenet_class_index.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cc66ac320026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;31m# Load label texts for ImageNet predictions so we know what model is predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0midx2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dataset/imagenet/imagenet_class_index.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0midx2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/qing/wind/MFPP/dataset/imagenet/imagenet_class_index.json'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import cv2,math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm,trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from skimage.data import astronaut\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel\n",
    "from skimage.segmentation import slic,mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def mfpp(model,\n",
    "         input,\n",
    "         img_file,\n",
    "         target=None,\n",
    "         seed=0,\n",
    "         num_masks=20000,\n",
    "         resize_offset=2.0,\n",
    "         layer=5,\n",
    "         batch_size=32,\n",
    "         p_1=0.5,\n",
    "         resize_mode='bilinear'):\n",
    "    r\"\"\"MFPP.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`torch.nn.Module`): a model.\n",
    "        input (:class:`torch.Tensor`): input tensor.\n",
    "        seed (int, optional): manual seed used to generate random numbers.\n",
    "            Default: ``0``.\n",
    "        num_masks (int, optional): number of MFPP random masks to use.\n",
    "            Default: ``8000``.\n",
    "        resize_offset(float): the offset for resized image for crop. Default: ``2.5``.\n",
    "        layer (int): the number of segments style. Default: ``5``.\n",
    "        batch_size (int, optional): batch size to use. Default: ``128``.\n",
    "        p_1 (float, optional): with prob p_1, a low-res cell is set to 1;\n",
    "            otherwise, it's 1. Default: ``0.5``.\n",
    "        resize_mode (str, optional): If resize is not None, use this mode for\n",
    "            the resize function. Default: ``'bilinear'``.\n",
    "\n",
    "    Returns:\n",
    "        :class:`torch.Tensor`: MFPP saliency map.\n",
    "    \"\"\"\n",
    "    SEG_COEFF=50 #relationship between segmente number and index. It's fixed to 50.\n",
    "#     response = requests.get(url)\n",
    "# #     print(\"response\",response)\n",
    "#     img = Image.open(BytesIO(response.content))\n",
    "    img = Image.open(img_file)\n",
    "    img=img.resize((224,224))\n",
    "\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "\n",
    "    segments=[[],[]]*5\n",
    "    n_features=[[]]*5  \n",
    "    \n",
    "#     plt.figure(figsize=(10,40),dpi=120) \n",
    "    for i in range(0,layer):\n",
    "        num_segments=SEG_COEFF*(2**i)\n",
    "        print(\"n_segments:\",num_segments)\n",
    "        segments[i]=slic(img,n_segments=num_segments,compactness=10,sigma=1)\n",
    "        n_features[i] = np.unique(np.asarray(segments[i])).shape[0]\n",
    "        print (\"n_features: \",n_features[i] )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get device of input (i.e., GPU).\n",
    "        dev = input.device\n",
    "        # Initialize saliency mask and mask normalization term.\n",
    "        input_shape = input.shape\n",
    "        saliency_shape = list(input_shape)\n",
    "        print(input_shape)\n",
    "\n",
    "        height = input_shape[2]\n",
    "        width = input_shape[3]\n",
    "        \n",
    "        H=height + math.floor(resize_offset*height)\n",
    "        W=width + math.floor(resize_offset*width)\n",
    "\n",
    "        out = model(input)\n",
    "        num_classes = out.shape[1]\n",
    "\n",
    "        saliency_shape[1] = num_classes\n",
    "        saliency = torch.zeros(saliency_shape, device=dev)\n",
    "\n",
    "        # Save current random number generator state.\n",
    "        state = torch.get_rng_state()\n",
    "\n",
    "        # Set seed.\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        num_chunks = (num_masks + batch_size - 1) // batch_size\n",
    "#         print(\"num_chunks:\",num_chunks)\n",
    "        layer_group=num_chunks//layer\n",
    "#         print(\"layer_group:\",layer_group)\n",
    "        \n",
    "        for chunk in trange(num_chunks):\n",
    "#             print(\"chunk:\",chunk)\n",
    "            # Generate MFPP random masks on the fly.\n",
    "            mask_bs = min(num_masks - batch_size * chunk, batch_size)\n",
    "#             print(\"mask_bs \",mask_bs) #32\n",
    "            layer_index=chunk//layer_group\n",
    "#             print(\"layer_index:\",layer_index) #0~4\n",
    "            np_masks = np.zeros((mask_bs, ) + IMAGE_SIZE[:2], dtype=np.float32)\n",
    "#             print(\"init_masks.shape:\",np_masks.shape) # (32, 224, 224)\n",
    "            data= np.random.choice([0, 1], size=n_features[layer_index], p=[1 - p_1, p_1])\n",
    "#                 print(\"data:\",data)\n",
    "            zeros = np.where(data == 0)[0]\n",
    "#                 print(\"zeros:\",zeros)\n",
    "            mask = np.zeros(segments[layer_index].shape).astype(float)\n",
    "            for z in zeros:\n",
    "#               print(\"z:\",z)\n",
    "                mask[segments[layer_index] == z] = 1.0\n",
    "#               print(\"mask:\",mask)\n",
    "            mask= Image.fromarray(mask * 255.)\n",
    "#               plt.imshow(mask)\n",
    "\n",
    "            mask = mask.resize((H,W),Image.BILINEAR)\n",
    "            mask= np.array(mask)\n",
    "        \n",
    "            for i in range(mask_bs):#(32 masks)\n",
    "                # crop to HxW\n",
    "                w_crop = np.random.randint(0, resize_offset*width + 1)\n",
    "                h_crop = np.random.randint(0, resize_offset*height + 1)\n",
    "#                 print(\"w_crop h_crop:\",w_crop,h_crop)\n",
    "                np_masks[i] = mask[h_crop:height + h_crop, w_crop:width + w_crop]\n",
    "#                 print(\"mask.shape:\",mask.shape)\n",
    "#                 print(\"{}:{} {}:{}\",h_crop,height + h_crop, w_crop,width + w_crop)\n",
    "                if np.isnan(np.sum(np_masks[i])):\n",
    "                    np_masks[i] = np_masks[0].copy() if not np.isnan(np.sum(np_masks[0])) else np_masks[1].copy()\n",
    "#                 np_masks[i] /= np.max(np_masks[i])\n",
    "                np_masks[i] /= 255.0\n",
    "\n",
    "            masks = torch.from_numpy(np_masks)\n",
    "            masks=masks.to(dev)\n",
    "            masks = masks.resize(32,1,224,224)\n",
    "\n",
    "            # Accumulate saliency mask.\n",
    "            for i, inp in enumerate(input):\n",
    "                out = torch.sigmoid(model(inp.unsqueeze(0) * masks))\n",
    "                if len(out.shape) == 4:\n",
    "                    assert out.shape[2] == 1\n",
    "                    assert out.shape[3] == 1\n",
    "                    out = out[:, :, 0, 0]\n",
    "                sal = torch.matmul(out.data.transpose(0, 1),\n",
    "                                   masks.view(mask_bs, height * width))\n",
    "                sal = sal.view((num_classes, height, width))\n",
    "                saliency[i] = saliency[i] + sal\n",
    "        saliency /= num_masks\n",
    "\n",
    "        # Restore original random number generator state.\n",
    "        torch.set_rng_state(state)\n",
    "\n",
    "        return saliency\n",
    "\n",
    "###inference\n",
    "from utils import *\n",
    "\n",
    "# url= 'https://lh3.googleusercontent.com/proxy/sDYB4cDFBemUrYfd95Y9umcDd5b_XygAGYtY4gzKWKRKGtchmOMFcepBw7SmiJ_YVkXAvTKUYUbhc5aR3G3e7YqjLIFBPluIOEtnO6sXKTKkb-L1k52VhkIf39WHrzOWikG7Oc6dPJG9y8q0mpc'\n",
    "# model_name='vgg16'\n",
    "model_name='resnet50'\n",
    "# Obtain example data.\n",
    "\n",
    "img_file=\"samples/sample.jpg\"\n",
    "model, x= get_example_data(img_file,model_name)\n",
    "\n",
    "# Load label texts for ImageNet predictions so we know what model is predicting\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath('./dataset/imagenet/imagenet_class_index.json'), 'r') as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))} \n",
    "    \n",
    "# Predicitions we got are logits. Let's pass that through softmax to get probabilities and class labels for top 5 predictions.\n",
    "logits = model(x)\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)\n",
    "ids = [idx2label[probs5[1][0][0]], idx2label[probs5[1][0][1]], idx2label[probs5[1][0][2]],\n",
    "       idx2label[probs5[1][0][3]], idx2label[probs5[1][0][4]]]\n",
    "tuple((p, c, idx2label[c]) for p, c in zip(probs5[0][0].cpu().detach().numpy(), probs5[1][0].cpu().detach().numpy()))\n",
    "print(probs5[0][0].cpu().detach().numpy(), probs5[1][0].cpu().detach().numpy())\n",
    "print(ids)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,20),dpi=80)\n",
    "\n",
    "# Plot input image\n",
    "img=Image.open(img_file)\n",
    "img=img.resize((224,224))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('input image', fontsize=18)\n",
    "plt.imshow(img)\n",
    "\n",
    "# Plot topk predictions's saliency\n",
    "saliency =mfpp(model, x, img_file)\n",
    "\n",
    "for i in trange(5):\n",
    "    print(\"i:\",i)\n",
    "    category_id=probs5[1][0][i].cpu().detach().numpy()\n",
    "    category_name=ids[i]\n",
    "\n",
    "    heatmap= saliency[:, category_id].unsqueeze(0)\n",
    "    heatmap=heatmap.cpu().data.numpy().squeeze()\n",
    "    plt.subplot(2,3,i+2)\n",
    "    plt.title('MFPP for category {} ({})'.format(category_name, category_id), fontsize=18)\n",
    "    plt.imshow(heatmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptgpu37",
   "language": "python",
   "name": "ptgpu37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
